{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99828b1d",
   "metadata": {},
   "source": [
    "## 1. Imports, Environment, and Elasticsearch Client Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watsonx_wrapper import WatsonxWrapper\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters, TextGenDecodingMethod\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_environment_vars() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Loads environment variables from the parent directory .env file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary with Elasticsearch URL, username, and password.\n",
    "    \"\"\"\n",
    "    parent_dir: str = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "    dotenv_path: str = os.path.join(parent_dir, '.env')\n",
    "    load_dotenv(dotenv_path)\n",
    "    return {\n",
    "        \"ES_URL\": os.getenv('ELASTIC_URL'),\n",
    "        \"ES_User_Name\": os.getenv('ELASTIC_USERNAME'),\n",
    "        \"ES_Password\": os.getenv('ELASTIC_PASSWORD')\n",
    "    }\n",
    "\n",
    "env_vars: Dict[str, str] = load_environment_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a global client connection to Elasticsearch\n",
    "es_client: Elasticsearch = Elasticsearch(\n",
    "    env_vars[\"ES_URL\"], \n",
    "    basic_auth=(env_vars[\"ES_User_Name\"], env_vars[\"ES_Password\"]),\n",
    "    verify_certs=False,\n",
    "    request_timeout=10000\n",
    ")\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61e4f6",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions for Index and Field Operations\n",
    "We update the field extraction function to return both the field name and its data type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(es: Elasticsearch) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve all index names from Elasticsearch.\n",
    "\n",
    "    :param es: Elasticsearch client.\n",
    "    :return: List of index names.\n",
    "    \"\"\"\n",
    "    return list(es.indices.get_alias().keys())\n",
    "\n",
    "def get_fields(es: Elasticsearch, index: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Retrieve field details from an Elasticsearch index mapping.\n",
    "\n",
    "    :param es: Elasticsearch client.\n",
    "    :param index: Index name.\n",
    "    :return: List of dictionaries with field_name and data_type.\n",
    "    \"\"\"\n",
    "    mapping = es.indices.get_mapping(index=index)\n",
    "    properties = mapping[index]['mappings'].get('properties', {})\n",
    "    fields = []\n",
    "    for field, details in properties.items():\n",
    "        dtype = details.get('type', 'unknown')\n",
    "        fields.append({\"field_name\": field, \"data_type\": dtype})\n",
    "    return fields\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14935bf0",
   "metadata": {},
   "source": [
    "## 3. Define Function to Get Diverse Terms Using Aggregations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diverse_terms(es: Elasticsearch, index: str, field: str,data_type: str, size: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve diverse representative terms using multiple aggregations.\n",
    "\n",
    "    :param es: Elasticsearch client.\n",
    "    :param index: Index name.\n",
    "    :param field: Field name.\n",
    "    :param size: Number of terms to retrieve.\n",
    "    :return: Dictionary containing various term types.\n",
    "    \"\"\"\n",
    "    # If the field is a text type, use its keyword sub-field (if available)\n",
    "    agg_field = field + \".keyword\" if data_type == \"text\" else field\n",
    "    query = {\n",
    "        \"aggs\": {\n",
    "            \"frequent_terms\": {\n",
    "                \"terms\": {\n",
    "                    \"field\": agg_field,\n",
    "                    \"size\": size\n",
    "                }\n",
    "            },\n",
    "            \"rare_terms\": {\n",
    "                \"rare_terms\": {\n",
    "                    \"field\": agg_field,\n",
    "                    \"max_doc_count\": 5\n",
    "                }\n",
    "            },\n",
    "            \"significant_terms\": {\n",
    "                \"significant_terms\": {\n",
    "                    \"field\": agg_field,\n",
    "                    \"size\": size\n",
    "                }\n",
    "            },\n",
    "            \"unique_count\": {\n",
    "                \"cardinality\": {\n",
    "                    \"field\": agg_field\n",
    "                }\n",
    "            },\n",
    "            \"sample_docs\": {\n",
    "                \"top_hits\": {\n",
    "                    \"size\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es.search(index=index, body=query)\n",
    "    return {\n",
    "        \"frequent_terms\": [bucket[\"key\"] for bucket in response[\"aggregations\"][\"frequent_terms\"][\"buckets\"]],\n",
    "        \"rare_terms\": [bucket[\"key\"] for bucket in response[\"aggregations\"][\"rare_terms\"][\"buckets\"]],\n",
    "        \"significant_terms\": [bucket[\"key\"] for bucket in response[\"aggregations\"][\"significant_terms\"][\"buckets\"]],\n",
    "        \"unique_count\": response[\"aggregations\"][\"unique_count\"][\"value\"],\n",
    "        \"sample_docs\": response[\"aggregations\"][\"sample_docs\"][\"hits\"][\"hits\"]\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f14985",
   "metadata": {},
   "source": [
    "## 4. Initialize WatsonX Client for Column Description Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watsonx_client = WatsonxWrapper(\n",
    "    model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    params=TextGenParameters(\n",
    "        decoding_method=TextGenDecodingMethod.GREEDY,\n",
    "        max_new_tokens=1000,\n",
    "        min_new_tokens=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c05920",
   "metadata": {},
   "source": [
    "## 5. Define the Prompt Template and JSON Extraction Function\n",
    "We update our prompt to include a personality-driven system message, detailed instructions in the user prompt, and an exact JSON output format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA3_BASE_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{llm_prefix}\"\"\"\n",
    "\n",
    "# Define system prompt: personality and purpose\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a highly knowledgeable metadata dictionary agent. \"\n",
    "    \"Your mission is to help Elasticsearch admins understand their data by providing clear, concise natural language descriptions for each field.\"\n",
    ")\n",
    "\n",
    "# Define user prompt template with instructions, input data, and required JSON output format.\n",
    "USER_PROMPT_TEMPLATE = (\n",
    "    \"Analyze the field based on the input below:\\n\"\n",
    "    \"- Index Name: {index}\\n\"\n",
    "    \"- Field Name: {field}\\n\"\n",
    "    \"- Data Type: {data_type}\\n\"\n",
    "    \"- Sample Values: {samples}\\n\\n\"\n",
    "    \"Generate a description that explains the field's purpose. \"\n",
    "    \"Return the output strictly in the following JSON format (no additional text):\\n\\n\"\n",
    "    \"json\\n\"\n",
    "    \"{{\\n\"\n",
    "    '  \"field_name\": \"{field}\",\\n'\n",
    "    '  \"index_name\": \"{index}\",\\n'\n",
    "    '  \"data_type\": \"{data_type}\",\\n'\n",
    "    '  \"natural_language_description\": \"<your description>\",\\n'\n",
    "    '  \"sample_value\": \"{samples}\"\\n'\n",
    "    \"}}\\n\"\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Define LLM prefix text\n",
    "LLM_PREFIX = \"**Generating Metadata Dictionary in desired format:**\"\n",
    "\n",
    "def generate_prompt(index: str, field: str, data_type: str, samples: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate the full prompt using the base prompt template.\n",
    "\n",
    "    :param index: Index name.\n",
    "    :param field: Field name.\n",
    "    :param data_type: Data type of the field.\n",
    "    :param samples: Sample values from the field.\n",
    "    :return: The complete prompt string.\n",
    "    \"\"\"\n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        index=index,\n",
    "        field=field,\n",
    "        data_type=data_type,\n",
    "        samples=', '.join(map(str, samples))\n",
    "    )\n",
    "    return LLAMA3_BASE_PROMPT.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        user_prompt=user_prompt,\n",
    "        llm_prefix=LLM_PREFIX\n",
    "    )\n",
    "\n",
    "def extract_json(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract a JSON object from the text using regex.\n",
    "\n",
    "    :param text: The text generated by the LLM.\n",
    "    :return: Parsed JSON as a dictionary.\n",
    "    :raises ValueError: If no JSON object can be found.\n",
    "    \"\"\"\n",
    "    # This regex finds the first JSON-like structure between curly braces.\n",
    "    pattern = r\"(\\{(?:.|\\n)*\\})\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Error parsing JSON: {e}\")\n",
    "    raise ValueError(\"No JSON object found in the text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a40184",
   "metadata": {},
   "source": [
    "## 6. Test for a Single Index and a Single Field\n",
    "Here we select a specific index and field, generate the prompt, call the LLM, and then extract the JSON output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select test index and field details\n",
    "test_index = 'employee_data'\n",
    "fields_info = get_fields(es_client, test_index)\n",
    "if not fields_info:\n",
    "    raise ValueError(f\"No fields found in index {test_index}.\")\n",
    "\n",
    "# Use the first field for testing\n",
    "test_field_info = fields_info[0]\n",
    "test_field = test_field_info[\"field_name\"]\n",
    "data_type = test_field_info[\"data_type\"]\n",
    "\n",
    "# Get diverse terms for the test field\n",
    "diverse_terms = get_diverse_terms(es_client, test_index, test_field,data_type)\n",
    "combined_samples = list(set(\n",
    "    diverse_terms[\"frequent_terms\"] +\n",
    "    diverse_terms[\"rare_terms\"] +\n",
    "    diverse_terms[\"significant_terms\"]\n",
    "))[:10]  # limit to 10 samples\n",
    "\n",
    "# Generate the full prompt\n",
    "prompt = generate_prompt(test_index, test_field, data_type, combined_samples)\n",
    "print(\"=== Generated Prompt ===\")\n",
    "print(prompt)\n",
    "\n",
    "# Call the LLM\n",
    "llm_output = watsonx_client.generate_text(prompt=prompt).strip()\n",
    "print(\"\\n=== Raw LLM Output ===\")\n",
    "print(llm_output)\n",
    "\n",
    "# Extract JSON from the LLM output\n",
    "try:\n",
    "    json_output = extract_json(llm_output)\n",
    "    print(\"\\n=== Extracted JSON ===\")\n",
    "    print(json.dumps(json_output, indent=4))\n",
    "except ValueError as e:\n",
    "    print(f\"Error extracting JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edeef5",
   "metadata": {},
   "source": [
    "## 7. Multithreaded Processing Over Fields in an Index\n",
    "This cell processes multiple fields concurrently in the selected index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_field(es: Elasticsearch, index: str, field_detail: Dict[str, str],\n",
    "                  llm_client: WatsonxWrapper, threshold: int = 20) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single field from an Elasticsearch index to generate metadata.\n",
    "\n",
    "    :param es: Elasticsearch client.\n",
    "    :param index: Index name.\n",
    "    :param field_detail: Dictionary containing field_name and data_type.\n",
    "    :param llm_client: WatsonX LLM client.\n",
    "    :param threshold: Maximum number of sample values to use.\n",
    "    :return: Dictionary with metadata for the field.\n",
    "    \"\"\"\n",
    "    field = field_detail[\"field_name\"]\n",
    "    data_type = field_detail[\"data_type\"]\n",
    "    diverse_terms = get_diverse_terms(es, index, field,data_type=data_type,size=threshold)\n",
    "    combined_samples = list(set(\n",
    "        diverse_terms[\"frequent_terms\"] +\n",
    "        diverse_terms[\"rare_terms\"] +\n",
    "        diverse_terms[\"significant_terms\"]\n",
    "    ))[:threshold]\n",
    "    prompt = generate_prompt(index, field, data_type, combined_samples)\n",
    "    llm_text = llm_client.generate_text(prompt=prompt).strip()\n",
    "    \n",
    "    try:\n",
    "        result_json = extract_json(llm_text)\n",
    "    except ValueError:\n",
    "        result_json = {\n",
    "            \"field_name\": field,\n",
    "            \"index_name\": index,\n",
    "            \"data_type\": data_type,\n",
    "            \"natural_language_description\": llm_text,\n",
    "            \"sample_value\": combined_samples[0] if combined_samples else \"\"\n",
    "        }\n",
    "    \n",
    "    return result_json\n",
    "\n",
    "# Process all fields in a test index concurrently\n",
    "fields_info = get_fields(es_client, test_index)\n",
    "metadata_fields: List[Dict[str, Any]] = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(process_field, es_client, test_index, field_info, watsonx_client): field_info for field_info in fields_info}\n",
    "    for future in as_completed(futures):\n",
    "        metadata_fields.append(future.result())\n",
    "\n",
    "print(\"=== Metadata for each field in index:\", test_index, \"===\")\n",
    "print(json.dumps(metadata_fields, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/metadata.json', 'w') as f:    \n",
    "    json.dump(metadata_fields, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3d237",
   "metadata": {},
   "source": [
    "## 8. Full Metadata Dictionary Generation Over All Indices\n",
    "This final cell processes every index using multithreaded field processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def process_index(es: Elasticsearch, index: str, llm_client: Any, threshold: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process an Elasticsearch index to generate a metadata dictionary for each field.\n",
    "\n",
    "    :param es: Elasticsearch client.\n",
    "    :param index: Index name.\n",
    "    :param llm_client: WatsonX LLM client.\n",
    "    :param threshold: Maximum number of sample values per field.\n",
    "    :return: List of field metadata dictionaries.\n",
    "    \"\"\"\n",
    "    fields_info = get_fields(es, index)\n",
    "    metadata: List[Dict[str, Any]] = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(process_field, es, index, field_info, llm_client, threshold): field_info for field_info in fields_info}\n",
    "        for future in as_completed(futures):\n",
    "            metadata.append(future.result())\n",
    "    return metadata\n",
    "\n",
    "def generate_metadata_dictionary(es: Elasticsearch, llm_client: Any, threshold: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a full metadata dictionary for all Elasticsearch indices.\n",
    "\n",
    "    :param es: Elasticsearch client.\n",
    "    :param llm_client: WatsonX LLM client.\n",
    "    :param threshold: Maximum number of sample values per field.\n",
    "    :return: List of metadata entries.\n",
    "    \"\"\"\n",
    "    indices = get_indices(es)\n",
    "    metadata_dict: List[Dict[str, Any]] = []\n",
    "    for index in indices:\n",
    "        metadata_dict.extend(process_index(es, index, llm_client, threshold))\n",
    "    return metadata_dict\n",
    "\n",
    "# Generate metadata dictionary for all indices\n",
    "full_metadata_dict = generate_metadata_dictionary(es_client, watsonx_client)\n",
    "print(\"=== Full Metadata Dictionary ===\")\n",
    "print(json.dumps(full_metadata_dict, indent=4))\n",
    "\n",
    "# Optionally, save the full metadata dictionary to a JSON file\n",
    "with open(\"es_full_metadata.json\", \"w\") as file:\n",
    "    json.dump(full_metadata_dict, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
